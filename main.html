<html>

<head>
    <meta charset="utf-8" />
    <title>VIRAL: Visual Representation Alignment for Multimodal Large Language Models</title>

    <!-- Favicon references -->
    <link rel="icon" type="image/x-icon" href="favicon.ico">
    <link rel="icon" type="image/jpeg" href="favicon.jpg">
    <link rel="apple-touch-icon" href="favicon.jpg">

    <meta
        content="Describe Anything: Detailed Localized Image and Video Captioning"
        name="description" />
    <meta
        content="VIRAL: Visual Representation Alignment for Multimodal Large Language Models"
        property="og:title" />
    <meta
        content="VIRAL: Visual Representation Alignment for Multimodal Large Language Models"
        property="og:description" />
    <meta content="https://describe-anything.github.io/images/slideshow/slide1_full.jpg" property="og:image" />
    <meta
        content="VIRAL: Visual Representation Alignment for Multimodal Large Language Models"
        property="twitter:title" />
    <meta
        content="VIRAL: Visual Representation Alignment for Multimodal Large Language Models"
        property="twitter:description" />
    <!-- <meta content="https://describe-anything.github.io/images/slideshow/slide1_full.jpg" property="twitter:image" /> -->
    <meta property="og:type" content="website" />
    <meta content="summary_large_image" name="twitter:card" />
    <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"
        crossorigin="anonymous">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&family=Varela+Round&display=swap"
        rel="stylesheet">
    <link href="style.css?" rel="stylesheet" type="text/css" />
    <!-- MathJax for LaTeX rendering -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$','$$'], ['\\[','\\]']]
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            },
            svg: { fontCache: 'global' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" async></script>
</head>

<body>
    <header class="site-header">
        <div class="container">
            <nav class="main-nav">
                <ul class="nav-links">
                    <li><a href="#abstract">Overview</a></li>
                    <li><a href="#pilot-study">Pilot study</a></li>
                    <li><a href="#framework">Framework</a></li>
                    <li><a href="#visual-representation-alignment">Methods</a></li>
                    <li><a href="#ablation-studies">Ablation Studies</a></li>
                    <li><a href="#analysis">Analysis</a></li>
                    <li><a href="#citation">Citation</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <div class="hero-section">
        <div class="container">
            <div class="title-row">
                <h1 class="title"><span class="gradient-text">VIRAL</span>: Visual Representation Alignment for Multimodal Large Language Models</h1>
                <h1 class="subtitle">arXiv 2025</h1>
            </div>
            <div class="base-row author-row">
                <div class="base-col author-col">
                    <a target="_blank" class="author-text">
                        Heeji Yoon<sup>1</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://crepejung00.github.io/" target="_blank" class="author-text">
                        Jaewoo Jung<sup>1</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://junwankimm.github.io/" target="_blank" class="author-text">
                        Junwan Kim<sup>1</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://sifeiliu.net/" target="_blank" class="author-text">
                        Sifei Liu<sup>1</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://hanzimao.me/" target="_blank" class="author-text">
                        Hanzi Mao<sup>1</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://sites.google.com/site/boyilics/home" target="_blank" class="author-text">
                        Boyi Li<sup>1,2</sup>
                    </a>
                </div>
            </div>
            <div class="base-row author-row">
                <div class="base-col author-col">
                    <a href="https://research.nvidia.com/person/marco-pavone" target="_blank" class="author-text">
                        Marco Pavone<sup>1</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://mingyuliu.net/" target="_blank" class="author-text">
                        Ming-Yu Liu<sup>1</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank" class="author-text">
                        Trevor Darrell<sup>2</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://www.adamyala.org/" target="_blank" class="author-text">
                        Adam Yala<sup>2,3</sup>
                    </a>
                </div>
                <div class="base-col author-col">
                    <a href="https://ycui.me/" target="_blank" class="author-text">
                        Yin Cui<sup>1</sup>
                    </a>
                </div>
            </div>
            <div class="base-row author-row">
                <div class="base-col author-col affiliations">
                    <sup>1</sup>KAIST AI &nbsp;&nbsp; <sup>2</sup>UC Berkeley &nbsp;&nbsp; <sup>3</sup>UCSF
                </div>
            </div>
            <div class="link-labels base-row">
                <div class="base-col icon-col"><a href="" target="_blank"
                        class="link-block">
                        <i class="fa fas fa-file-text main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Paper</strong>
                    </a></div>
                <div class="base-col icon-col"><a href='' class="link-block">
                        <i class="fa fa-github main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Code</strong>
                    </a></div>
                <div class="base-col icon-col"><a href="#citation" class="link-block">
                        <i class="fa fa-graduation-cap main-icon" style="font-size: 60px"></i>
                        <strong class="link-labels-text">Citation</strong>
                    </a></div>
            </div>

        </div>
    </div>

    <main class="main-content">
        <div class="container">
            <div class="tldr">
                <b>TL;DR</b>: VIsual Representation ALignment (VIRAL) is a simple but effective regularization strategy that aligns the internal visual representations of MLLMs with those from pretrained vision foundation models.
            </div>

            <!--<section id="additional-examples" class="section" style="margin: -1rem 0 -2rem 0;">
                <h2>High-quality Captions in Challenging Scenes</h2>

                <p>Our model is able to generate high-quality detailed localized captions for diverse regions, including the main objects, small objects in the background, and regions in cluttered scenes.</p>

                <div class="slideshow-container">
                    <div class="slideshow">
                        <div class="slide">
                            <img src="images/examples/1.jpg" class="slideshow-image">
                            <div class="image-caption">
                                <b>Caption from DAM:</b> A medium-sized dog with a thick, reddish-brown coat and a white underbelly and legs. The dog has a bushy tail, pointed ears, and a red collar with a silver tag. Its mouth is open, showing its teeth, and its tongue is hanging out. The dog is in a running pose with its front legs extended forward and its back legs stretched out behind.
                            </div>
                        </div>
                        <div class="slide">
                            <video src="images/examples/8.mp4" class="slideshow-image-video" autoplay muted loop></video>
                            <div class="image-caption">
                                <b>Caption from DAM:</b> A vibrant goldfish with a striking pattern of orange and black spots glides gracefully through the water. Its body is elongated and streamlined, with a prominent dorsal fin that arches elegantly along its back. The tail fin fans out, displaying a delicate, translucent quality that ripples with each movement. As it swims, the goldfish's body undulates smoothly, propelling it forward with a gentle, rhythmic motion. The fins, particularly the pectoral fins, flutter subtly, aiding in navigation and balance. The goldfish's head is slightly pointed, with a curious, expressive eye that seems to observe its surroundings with keen interest. Throughout its journey, the goldfish maintains a steady, fluid motion, exuding a sense of calm and elegance as it interacts with the aquatic environment.
                            </div>
                        </div>
                        <div class="slide">
                            <img src="images/examples/2.jpg" class="slideshow-image">
                            <div class="image-caption">
                                <b>Caption from DAM:</b> A blue wooden chair with a straight backrest featuring three vertical slats. The seat is flat and rectangular, and the chair has four sturdy legs.
                            </div>
                        </div>
                        <div class="slide">
                            <img src="images/examples/3.jpg" class="slideshow-image">
                            <div class="image-caption">
                                <b>Caption from DAM:</b> A round, green metal table with a flat, circular top. The table has a cylindrical base with a horizontal black band around its middle. The base is supported by four black metal legs.
                            </div>
                        </div>
                        <div class="slide">
                            <img src="images/examples/4.jpg" class="slideshow-image">
                            <div class="image-caption">
                                <b>Caption from DAM:</b> The balcony features an ornate wrought iron railing with intricate floral and geometric patterns. The design includes symmetrical motifs with a central floral design flanked by symmetrical, ornate scrollwork. The railing is composed of vertical and horizontal bars, creating a grid-like structure with decorative elements.
                            </div>
                        </div>
                        <div class="slide">
                            <img src="images/examples/5.jpg" class="slideshow-image">
                            <div class="image-caption">
                                <b>Caption from DAM:</b> A white cat with light orange ears and a pink nose. The cat has a relaxed expression with slightly closed eyes and a soft, white fur coat.
                            </div>
                        </div>
                        <div class="slide">
                            <video src="images/examples/9.mp4" class="slideshow-image-video" autoplay muted loop></video>
                            <div class="image-caption">
                                <b>Caption from DAM:</b> A monkey with a light brown coat and a slightly darker face is seen in a series of dynamic movements. Initially, it appears to be reaching into a container with its right hand, which is holding a piece of yellow food. The monkey's posture is slightly hunched, indicating focus and intent as it interacts with the food. As the sequence progresses, the monkey brings the food closer to its mouth, using both hands to manipulate it. Its facial expression suggests concentration and enjoyment, with its eyes partially closed. The monkey's body shifts slightly, maintaining balance as it continues to eat. Throughout the sequence, the monkey's movements are fluid and purposeful, showcasing its dexterity and agility. The final frames depict the monkey holding the food with both hands, bringing it closer to its face, and then lowering it slightly, possibly preparing to take another bite.
                            </div>
                        </div>
                        <div class="slide">
                            <img src="images/examples/6.jpg" class="slideshow-image">
                            <div class="image-caption">
                                <b>Caption from DAM:</b> The eye has a dark, almost black pupil surrounded by a thick, irregularly shaped iris. The iris is predominantly light gray with patches of darker gray and brown, giving it a mottled appearance. The outer edge of the iris is slightly jagged and uneven.
                            </div>
                        </div>
                        <div class="slide">
                            <img src="images/examples/7.jpg" class="slideshow-image">
                            <div class="image-caption">
                                <b>Caption from DAM:</b> A modern skyscraper with a sleek, rectangular design featuring a series of vertical, evenly spaced windows. The building has a stepped structure, with each section slightly smaller than the one below it, creating a tiered effect. The facade is predominantly composed of reflective glass panels, giving it a contemporary and polished appearance.
                            </div>
                        </div>
                        <div class="slide">
                            <video src="images/examples/10.mp4" class="slideshow-image-video" autoplay muted loop></video>
                            <div class="image-caption">
                                <b>Caption from DAM:</b> A cow with a rich brown coat and a lighter patch on its rump is depicted in a sequence of movements. Initially, the cow is seen with its head slightly lowered, suggesting a calm demeanor. As the sequence progresses, the cow begins to move forward, its legs extending in a steady, rhythmic gait. The tail, with its tufted end, sways gently with each step, adding a sense of fluidity to its motion. The cow's body remains mostly upright, with its back slightly arched, indicating a relaxed posture. The legs, sturdy and well-defined, carry the cow forward with a sense of purpose. Throughout the sequence, the cow maintains a consistent pace, its movements smooth and unhurried, embodying a serene and composed presence.
                            </div>
                        </div>
                    </div>
                    
                    <button class="slideshow-nav prev" onclick="changeSlide(-1)">❮</button>
                    <button class="slideshow-nav next" onclick="changeSlide(1)">❯</button>
                    
                    <div class="slideshow-controls">
                        <button class="play-pause" onclick="togglePlayPause()">
                            <i class="fa fa-pause"></i>
                        </button>
                    </div>
                </div>
            </section> -->

            <div id="abstract" class="base-row section">
                <h2>Introducing the VIsual Representation Alignment (VIRAL)</h2>
                <p class="paragraph">
                    We propose a novel framework, <strong>VIsual Representation ALignment (VIRAL)</strong>, which introduces an auxiliary regularization objective on visual features to prevent MLLMs from discarding fine-grained visual attributes during training. VIRAL consistently produces more accurate visually grounded responses and yields substatntial improvements over standard visual instruction tuning baselines when employing diverse vision towers, including CLIP and SigLIPv2.
                </p>
            </div>

            <div class="image-container">
                    <div class="image-content">
                        <img src="images/viral_teaser.png" class="img large-image">
                    </div>

                    <p class="image-caption">We propose a novel framework, <strong>VIsual Representation ALignment (VIRAL)</strong>, which introduces an auxiliary regularization objective on visual features to prevent MLLMs from discarding fine-grained visual attributes during training. VIRAL consistently produces more accurate visually grounded responses and yields substatntial improvements over standard visual instruction tuning baselines when employing diverse vision towers, including CLIP and SigLIPv2.<br/> </p>
            </div>

            

            <section id="pilot-study" class="section">
                <h2>Pilot study</h2>

                <p>Paragraph of pilot study: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua</p>
                <div class="image-container">
                    <div class="image-content">
                        <img src="images/viral_pilot_study.png" class="img large-image">
                    </div>
                    <p class="image-caption">Pilot study caption: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua </p>
                </div>
            </section>

            <section id="framework" class="section">
                <h2>Illustration of our framework</h2>

                <p>Building upon our findings in visual representation alignment, we align visual tokens from MLLM to strong, informative representations from VFMs.</p>
                <div class="image-container">
                    <img src="images/viral_architecture.png" class="image-item img large-image z-depth-1">
                    <p class="image-caption">Building upon our findings in visual representation alignment, we align visual tokens from MLLM to strong, informative representations from VFMs.</p>
                </div>
            </section>

            <section id="visual-representation-alignment" class="section">
                <h2>Effect of visual representation alignment</h2>

                <p>We report results of models trained with and without $\mathcal{L}_{\text{VRA}}$ across different vision encoders, evaluated on both vision-centric and general multimodal benchmarks. $\mathcal{L}_{\text{VRA}}$ consistently improves performance regardless of the encoder.</p>
                <div class="image-container">
                    <img src="images/viral_alignment_table.png" class="img large-image">
                    <p class="image-caption">We report results of models trained with and without $\mathcal{L}_{\text{VRA}}$ across different vision encoders, evaluated on both vision-centric and general multimodal benchmarks. $\mathcal{L}_{\text{VRA}}$ consistently improves performance regardless of the encoder.</p>
                </div>

                <p>The left part presents PCA visualizations of intermediate representations, demonstrating that VIRAL yields more structured, semantically meaningful visual embeddings. The right part illustrates instance counting and spatial relation tasks, highlighting scenarios where VIRAL correctly answers questions while the baseline fails.</p>
                <div class="image-container">
                    <img src="images/viral_qual.png" class="img large-image">
                    <p class="image-caption">The left part presents PCA visualizations of intermediate representations, demonstrating that VIRAL yields more structured, semantically meaningful visual embeddings. The right part illustrates instance counting and spatial relation tasks, highlighting scenarios where VIRAL correctly answers questions while the baseline fails.</p>
                </div>
            </section>

            <section id="ablation-studies" class="section">
                <h2>Ablation study on key design components</h2>

                <p>We analyze the effects of (i) different vision foundation models (VFMs), (ii) alignment target layers, and (iii) alignment objectives, through evaluation on both vision-centric and general multimodal benchmarks. All experiments are conducted on the LLaVA-1.5-7B baseline.</p>
                <div class="image-container">
                    <img src="images/viral_ablation_study.png" class="img large-image">
                    <p class="image-caption">We analyze the effects of (i) different vision foundation models (VFMs), (ii) alignment target layers, and (iii) alignment objectives, through evaluation on both vision-centric and general multimodal benchmarks. All experiments are conducted on the LLaVA-1.5-7B baseline.</p>
                </div>
            </section>

            <section id="analysis" class="section">
                <h2>Analysis</h2>

                <p>Qualitative comparison on text-to-image attention maps (left) and quantified spatial entropy of attention across layers and heads (right). Applying visual representation alignment encourages model to attend to more contextually important content, yielding a more focused and structured attention pattern.</p>
                <div class="image-container">
                    <img src="images/viral_attention_anlaysis.png"
                        class="image-item img large-image z-depth-1">
                    <p class="image-caption">Qualitative comparison on text-to-image attention maps (left) and quantified spatial entropy of attention across layers and heads (right). Applying visual representation alignment encourages model to attend to more contextually important content, yielding a more focused and structured attention pattern.</p>
                </div>
                
                 <p>Performance with and without $\mathcal{L}_\mathrm{VRA}$ evaluated every 1K steps on (a) POPE and (b) CV-Bench$^\mathrm{2D}$. Improved early-stage performance with $\mathcal{L}_\mathrm{VRA}$ indicates that VIRAL facilitates faster convergence.</p>
                <div class="image-container">
                    <img src="images/viral_training_efficiency.png"
                        class="image-item img large-image z-depth-1">
                    <p class="image-caption">Performance with and without $\mathcal{L}_\mathrm{VRA}$ evaluated every 1K steps on (a) POPE and (b) CV-Bench$^\mathrm{2D}$. Improved early-stage performance with $\mathcal{L}_\mathrm{VRA}$ indicates that VIRAL facilitates faster convergence.</p>
                </div>

                 <p>Number of correct predictions out of 788 spatial reasoning tasks in CV-Bench$^\mathrm{2D}$. Models with $\mathcal{L}_\mathrm{VRA}$ show larger performance drops under random permutation, indicating stronger sensitivity to spatial relationships.</p>
                <div class="image-container">
                    <img src="images/viral_permutation.png"
                        class="image-item img large-image z-depth-1">
                    <p class="image-caption">Number of correct predictions out of 788 spatial reasoning tasks in CV-Bench$^\mathrm{2D}$. Models with $\mathcal{L}_\mathrm{VRA}$ show larger performance drops under random permutation, indicating stronger sensitivity to spatial relationships.</p>
                </div>
            </section>

            <section id="conclusion" class="section">
                <h2>Conclusion</h2>

                <p>In this work, we propose \ours, a simple yet effective training strategy that aligns the internal visual representations of MLLMs with those from powerful vision foundation models. Our approach preserves fine-grained visual semantics often discarded under text-only supervision, enabling more accurate spatial reasoning and object grounding. Extensive experiments across diverse benchmarks validate the effectiveness and generality of our method, demonstrating that visual representation alignment significantly enhances both performance and training efficiency in multimodal learning.</p>
            </section>

            <div class="citation add-top-padding">
                <h1 id="citation">Citation</h1>
                <p> If you use this work or find it helpful, please consider citing: </p>
                <pre id="codecell0"></pre>
            </div>
        </div>
    </main>

    <footer class="site-footer">
        <div class="container">
            <p class="credit">Credit: The design of this project page is inspired by previous academic project pages, such as <a href="https://llm-grounded-diffusion.github.io/" target="_blank">LLM-grounded Diffusion</a>.</p>
        </div>
    </footer>

    <script>
    function toggleMute(element) {
        const video = element.parentElement.querySelector('video');
        const icon = element.querySelector('i');
        const text = element.querySelector('.unmute-text');
        
        if (video.muted) {
            video.muted = false;
            icon.className = 'fa fa-volume-up';
            text.textContent = 'Mute';
        } else {
            video.muted = true;
            icon.className = 'fa fa-volume-off';
            text.textContent = 'Click to unmute';
        }
    }
    
    document.addEventListener('DOMContentLoaded', function() {
        const videos = document.querySelectorAll('video');
        videos.forEach(video => {
            video.addEventListener('play', function() {
                const overlay = this.parentElement.querySelector('.unmute-overlay');
                if (overlay) overlay.style.opacity = '0.8';
            });
            
            video.addEventListener('pause', function() {
                const overlay = this.parentElement.querySelector('.unmute-overlay');
                if (overlay) overlay.style.opacity = '0.8';
            });
        });

        // Initialize all slideshows
        document.querySelectorAll('.slideshow-container').forEach(container => {
            const slideshow = container.querySelector('.slideshow');
            const slides = slideshow.querySelectorAll('.slide');
            const prevButton = container.querySelector('.slideshow-nav.prev');
            const nextButton = container.querySelector('.slideshow-nav.next');
            const playPauseButton = container.querySelector('.play-pause');
            
            let currentSlide = 0;
            let autoplayInterval;
            let isPlaying = true;

            function showSlide(n) {
                slides.forEach(slide => slide.classList.remove('active'));
                currentSlide = (n + slides.length) % slides.length;
                slides[currentSlide].classList.add('active');
            }

            function changeSlide(n) {
                showSlide(currentSlide + n);
                resetAutoplay();
            }

            function togglePlayPause() {
                if (isPlaying) {
                    clearInterval(autoplayInterval);
                    playPauseButton.innerHTML = '<i class="fa fa-play"></i>';
                } else {
                    startAutoplay();
                    playPauseButton.innerHTML = '<i class="fa fa-pause"></i>';
                }
                isPlaying = !isPlaying;
            }

            function startAutoplay() {
                autoplayInterval = setInterval(() => {
                    showSlide(currentSlide + 1);
                }, 5000);
            }

            function resetAutoplay() {
                clearInterval(autoplayInterval);
                if (isPlaying) {
                    startAutoplay();
                }
            }

            // Initialize this slideshow
            showSlide(0);
            startAutoplay();

            // Add event listeners
            prevButton.addEventListener('click', () => changeSlide(-1));
            nextButton.addEventListener('click', () => changeSlide(1));
            playPauseButton.addEventListener('click', togglePlayPause);
        });

        // Handle main video play button
        const mainVideo = document.querySelector('.main-video');
        const playButton = document.querySelector('.play-button-overlay');
        
        if (mainVideo && playButton) {
            // Click play button to play video
            playButton.addEventListener('click', () => {
                mainVideo.play();
                mainVideo.classList.add('playing');
            });

            // Handle video play/pause events
            mainVideo.addEventListener('play', () => {
                mainVideo.classList.add('playing');
            });

            mainVideo.addEventListener('pause', () => {
                mainVideo.classList.remove('playing');
            });

            mainVideo.addEventListener('ended', () => {
                mainVideo.classList.remove('playing');
            });
        }
    });
    </script>
</body>
</html>